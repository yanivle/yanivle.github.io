---
layout: post
title:  "Smashing Images for Fun and Profit"
date:   2024-07-27 01:00:00
excerpt: "Some cool techniques for editing images and videos."
categories: AI
tags:  AI Computing Programming Hacking Python Math Puzzles Graphics
image:
  feature: face0_hero.png
bgContrast: dark
bgGradientOpacity: darker
syntaxHighlighter: no
---

I have a very bad memory.
Still, I vividly remember the mind-blowing moment of seeing the avocado-chair images from the Dall-E paper for the first time:

{% include image.html url="/assets/images/posts/image_gen/dalle1_avocado_chairs_white.jpeg" height=7 %}

Wow! Unbelievable that these are from almost four years ago!

Image generation is such a cool field, and I'm lucky to have spent a bit of time doing research here.
In this post I'll look back at some of my own work in image generation.

# UniTune

<!-- UniTune was a super fun hacking project for a couple of weeks, but to properly explain it, let's start with some important background here. -->

<!-- I was very lucky, a couple of years back, to come across the amazing work by Jonathan Ho, Mohammad Nourouzi, and others, including their amazing work on Imagen (who are now creating one of the coolest image generation platforms out there - [ideogram](https://ideogram.ai/)!) -->

A couple of years ago, some friends at Google developed a super cool technique known as [DreamBooth](https://dreambooth.github.io/).
To understand DreamBooth, consider the following problem - if I ask an image generation model to generate a "`Brad Pitt Pirate`" it does a great job, but if I ask it for a "`Yaniv Leviathan Pirate`" it does not:

{% include figure.html url="/assets/images/posts/image_gen/brad_pitt_yaniv_leviathan_pirates.jpeg" caption="Images generated by Stable Diffusion 2.1 for the prompts `Brad Pitt Pirate` (left) and `Yaniv Leviathan Pirate` (right)." %}

Why is that so? The only difference between Brad Pitt and me (_well, the only difference in this specific context, I'm sure you could find a couple more differences in general :)_) is that the model encountered many examples of Brad Pitt during training, while it likely didn't encounter any examples of me.

{% include figure.html url="/assets/images/posts/image_gen/image_gen brad pitt.png" caption="A sketch of the training data of an image generation model containing examples of Brad Pitt." height=5 %}

Well, DreamBooth suggested to fix just that - if we want our model to learn a new concept, such as to be able to draw me, let's just add examples of me (or whatever else we'd like the model to learn) to the training set and _train the model some more_.
Usually though, the images of Brad Pitt aren't all clumped up at the end of training, but are rather spread across the train set (see illustration above).
So during training, the model encounters them once in a while, while encountering a wide variety of other examples in between, so it doesn't forget how to draw other things (such as pirates).
We could just add a bunch of images of me to the original training set and re-train from scratch.
Unfortunately, that would be a very costly process.
Instead we'd rather just continue training some more.
To solve this, the DreamBooth authors suggested to train the model a bit more on a combination of a small sample from its original varied dataset and a bunch of images of our new subject (me in this example).

{% include figure.html url="/assets/images/posts/image_gen/image_gen dreambooth me.png" caption="A sketch showing the training data of the DreamBooth fine tuning process, teaching the model about me." height=7 %}

Et voil√† - the model learned a new concept!

{% include figure.html url="/assets/images/posts/image_gen/dreambooth_me.jpg" caption="Examples of images generated by Stable Diffusion v1.4 after training it a bit more on images of myself, DreamBooth style." %}

Usually btw, Dreambooth suggests to use _rare tokens_ (i.e. words that don't mean anything to the model yet) instead of the actual name of the new subject, to make learning easier and prevert further forgetting of known concepts. For example, my last name "Leviathan" might already carry some meaning for the model, so it might be harder for the model to learn that the word "Leviathan" following the word "Yaniv" means something different than the word "Leviathan" in other contexts. Also, DreamBooth suggests adding the _class type_ of the new subject to the prompts during fine tuning, so overall we'd get `"<RARE TOKENS> man"` for me (instead of `"Yaniv Leviathan"`).

As an aside, a couple of years back I wrote a small script that fetched the news headlines, converted them to prompts for Stable Diffusion (it basically tried to compress the entire headline into a single word, and then plug it into this fixed template `"portrait of <RARE TOKENS> man. {word}. highly detailed, male, digital painting, artstation, concept art, smooth, sharp focus, soft volumetric lights, illustration, cinematic lighting"`), generated images from a DreamBooth-ed model of myself, and displayed the results on my kitchen's Google Home, as an alternative way to consuming the news. I stopped this experiment very quickly (I admittedly didn't fully think about how horrible news headlines usually are...) but that's a story for another post.

{% include figure.html url="/assets/images/posts/image_gen/google home news me stable diffusion 2022 cropped.jpg" caption="My Google Home displaying a generated image for a (not so horrible) news article in 2022." %}

DreamBooth tried hard to not have the model memorize a specific image, but instead teach the model about a new concept (e.g. by including several images of the new subject, and interleaving them with varied other images).
But what happens if we do the opposite on purpose? I.e. we don't try to teach the model a new concept but instead try to get it to memorize a single specific image?
Well, it turns out you get _an image editor_!
I spent a couple of fun weeks in 2022 with Dani Valevski, Matan Kalman, and our other collaborators, experimenting with this idea, which we call [UniTune](https://arxiv.org/abs/2210.09477) (paper [here](https://arxiv.org/abs/2210.09477)).

For example, consider this photo of Matan and I (trivia detail: this is a photo from [the blog which introduced Google Duplex in 2018](https://research.google/blog/google-duplex-an-ai-system-for-accomplishing-real-world-tasks-over-the-phone/)):

{% include image.html url="/assets/images/posts/image_gen/Yaniv Matan Final.jpeg" %}

Here's how the training data might look like when UniTuning on this image:

{% include figure.html url="/assets/images/posts/image_gen/image_gen unitune.png" caption="A sketch of the training data of the UniTune fine tuning process, teaching the model about the image from the Google Duplex blog post." height=7 %}

Similarly to DreamBooth, we use rare tokens to make learning easier, and also similarly to DreamBooth, care is needed to prevent the model from forgetting existing concepts.

Once we have a UniTune model, the fun can begin! For example, if we'd like to create the teddy bears version of the image, we'd simply prompt the model with `"<RARE TOKENS> teddy bears"`:

{% include image.html url="/assets/images/posts/image_gen/unitune_hero.jpeg" height=20 %}

Here are some more examples:

{% include image.html url="/assets/images/posts/image_gen/unitune2.jpeg" height=20 %}

There's one important aspect I skipped over.
A _diffusion model_, such as Stable Diffusion, generates an image by starting from noise and gradually _denoising_ it.
Here's an example:

{% include figure.html url="/assets/images/posts/image_gen/image_gen pagoda.png" caption="An illustration of the denoising process through which a diffusion model generates an image of a pagoda." height=7 %}

But what if we want more control of the generated image? Say we want the pagoda to have certain colors or be located in specific areas of the image.
Making the prompt more verbose might help some, but this is clearly not sufficient (e.g. for finer color or location control).
Instead, in 2021, the authors of a really cool technique known as [SDEdit](https://arxiv.org/abs/2108.01073) suggested a simple but powerful idea - instead of starting the generation process from scratch, we can create a sketch of the desired end result, add some noise to it, and skip the first few denoising iterations.
Here's an example:

{% include figure.html url="/assets/images/posts/image_gen/image_gen pagoda sdedit.png" caption="SDEdit suggested starting from a sketch of the result, adding noise to it, and skipping the first few denoising steps." height=7 %}

In fact, the SDEdit authors suggested using their idea for editing images. Take the image you'd like to edit, make a sketch of the edit, noise it, and de-noise it again with a fixed prompt:

{% include figure.html url="/assets/images/posts/image_gen/image_gen pagoda sdedit edit.png" caption="An illustration of how an image might be edited with SDEdit (adding a red balloon in this case)." height=7 %}

Unfortunately, there is a catch - if we don't add enough noise (i.e. we start very late during denoising) the model might not have enough iterations to generate a high quality image. On the other hand, if we add too much noise, the model might decide to generate an image that is too dissimilar from the original image to edit. We could use a bunch of tricks, like letting the user specify some masks where the pixels of the original image will be forced, but this issue remains problematic with SDEdit alone.
In UniTune we found that a small amount SDEdit is beneficial, and that the combination of SDEdit and fine-tuning the model on a single image results in high quality results, without the user needing to specify masks.

# Dreamix

A few months after we published UniTune, together with Eyal Molad, Eliahu Horwitz, Dani Valevski, and other collaborators, we tried applying essentially the same method to edit videos, instead of editing images.
Turns out that is works well.
We named this method [Dreamix](https://dreamix-video-editing.github.io).

Here is an example:

{% include video.html url="https://dreamix-video-editing.github.io/static/videos/vid2vid_cats.mp4" %}

And another one:

{% include video.html url="https://dreamix-video-editing.github.io/static/videos/vid2vid_leaping.mp4" %}

Here, we are not fine-tuning an image generation model, but a video generation model instead.

In the video domain, we can do some other cool things, for example, we can start with an image, copy it in the time-axis (potentially applying some simple transformations, such as zooming-in), and create our source video to edit. This is essentially a simple technique for animating still images. Here's an example:

{% include video.html url="https://dreamix-video-editing.github.io/static/videos/img2vid_buffalo.mp4" %}

Finally, we can also fine tune the model on a collection of still images instead of on a movie. There is a small but important technicality here that I'm just going to gloss over here, but after we're done, we can get animated videos of subjects from a set of still images:

{% include video.html url="https://dreamix-video-editing.github.io/static/videos/subj_driv_lifting.mp4" %}

{% include video.html url="https://dreamix-video-editing.github.io/static/videos/subj_driv_walking.mp4" %}

You can find more details in [the paper](https://arxiv.org/abs/2302.01329).

# Face0

UniTune is a really simple idea that produces nice looking results, but it does suffer from a major weakness - while we are only fine-tuning our model, so we don't have to pay the high cost of pre-training, we are still training our model, even if just a bit. This training process is still costly, and can take many seconds or even minutes.

In our work [Face0](https://arxiv.org/abs/2306.06638) from a year ago, we suggested a different method for getting the model to generate an image in the likeness of a person _without needing to fine-tune_ the model.
The idea here is simple as well.

{% include image.html url="/assets/images/posts/image_gen/face0_hero.jpeg" height=20 %}

# RECAP

{% include image.html url="/assets/images/posts/image_gen/recap_hero.jpeg" height=20 %}

# GameNGen

