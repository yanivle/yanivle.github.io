{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab with all code snippet for [The Art of Transformer Programming book](https://yanivle.github.io//taotp.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 - The Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def norm(x, axis=-1, epsilon: float = 1e-10):\n",
    "  return x / (x.sum(axis=axis, keepdims=True) + epsilon)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "  return norm(np.exp(x - np.max(x, axis=axis, keepdims=True)), axis=axis)\n",
    "\n",
    "def layer_norm(x, gamma = 1., beta = 0., axis = -1, epsilon = 1e-10):\n",
    "  return beta + gamma * (x - x.mean(axis=axis, keepdims=True)) / (\n",
    "    x.var(axis=axis, keepdims=True) ** 0.5 + epsilon)\n",
    "\n",
    "def self_attn(x, Q, K, V, P, mask):\n",
    "  queries, keys, values = (np.einsum('nj,hjk->nhk', x, M) for M in (Q, K, V))\n",
    "  qk = np.einsum('nhk,mhk->nmh', queries, keys) / (Q.shape[-1] ** 0.5)\n",
    "  attn_weights = softmax(np.where(mask[..., None], qk, float('-inf')), axis=1)\n",
    "  x = np.einsum('nmh,mhk->nhk', attn_weights, values)\n",
    "  return np.einsum('nhk,hdk->nd', x, P)\n",
    "\n",
    "def transformer_layer(x, Q, K, V, P, M1, b1, M2, b2, ln1, ln2, mask):\n",
    "  x = x + self_attn(layer_norm(x, **ln1), Q, K, V, P, mask)\n",
    "  return x + np.maximum(layer_norm(x, **ln2) @ M1 + b1, 0) @ M2 + b2\n",
    "\n",
    "def transformer(tokens, tok_emb, pos_emb, out_emb, layers, lnf):\n",
    "  n, = tokens.shape\n",
    "  causal_mask = np.tril(np.ones((n, n)))\n",
    "  x = tok_emb[tokens] + pos_emb[np.arange(n)]\n",
    "  for layer in layers:\n",
    "    x = transformer_layer(x, **layer, mask=causal_mask)\n",
    "  return np.einsum('nd,vd->nv', layer_norm(x, **lnf), out_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokens, params, max_steps=1000, eos=None):\n",
    "  for _ in range(max_steps):\n",
    "    next_tok = np.argmax(transformer(tokens, **params)[-1])\n",
    "    tokens = np.append(tokens, next_tok)\n",
    "    if next_tok == eos: break\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_layer(n_heads, d_model, d_ff, d_head):\n",
    "    return {\n",
    "        'Q': np.zeros((n_heads, d_model, d_head)),\n",
    "        'K': np.zeros((n_heads, d_model, d_head)),\n",
    "        'V': np.zeros((n_heads, d_model, d_head)),\n",
    "        'P': np.zeros((n_heads, d_model, d_head)),\n",
    "        'M1': np.zeros((d_model, d_ff)),\n",
    "        'b1': np.zeros((d_ff,)),\n",
    "        'M2': np.zeros((d_ff, d_model)),\n",
    "        'b2': np.zeros((d_model,)),\n",
    "        'ln1': {'gamma': 1., 'beta': 0.},\n",
    "        'ln2': {'gamma': 1., 'beta': 0.},\n",
    "    }\n",
    "\n",
    "def base_params(vocab_size, block_size, d_model):\n",
    "    return {\n",
    "        'tok_emb': np.zeros((vocab_size, d_model)),\n",
    "        'out_emb': np.zeros((vocab_size, d_model)),\n",
    "        'pos_emb': np.zeros((block_size, d_model)),\n",
    "        'layers': [],\n",
    "        'lnf': {'gamma': 1., 'beta': 0.},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "class HumanReadableDict(dict):\n",
    "    def __repr__(self) -> str:\n",
    "        return \"{\" + \", \".join(f\"{k}: {v:,}\" for k, v in self.items()) + \"}\"\n",
    "\n",
    "def count_params(params, count=lambda x: x.size):\n",
    "    embs = [\"tok_emb\", \"pos_emb\"]\n",
    "    if not np.all(params[\"tok_emb\"] == params[\"out_emb\"]):\n",
    "        embs.append(\"out_emb\")\n",
    "    emb_params = {emb: count(params[emb]) for emb in embs}\n",
    "    non_emb_params = 0\n",
    "    for layer in params[\"layers\"]:\n",
    "        for param in layer.values():\n",
    "            if isinstance(param, np.ndarray):\n",
    "                non_emb_params += count(param)\n",
    "    return HumanReadableDict(\n",
    "        {\"n_params\": sum(emb_params.values()) + non_emb_params}\n",
    "        | emb_params\n",
    "        | {\"non_emb\": non_emb_params})\n",
    "\n",
    "count_non0_params = partial(count_params, count=lambda x: (x != 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaper 2 - Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleformer(seq, pos_emb, out_emb):\n",
    "  x = pos_emb[np.arange(len(seq))]\n",
    "  return np.einsum('nd,vd->nv', x, out_emb)\n",
    "\n",
    "def decode_simpleformer(tokens, pos_emb, out_emb, eos):\n",
    "  while True:\n",
    "    next_tok = np.argmax(simpleformer(tokens, pos_emb, out_emb)[-1])\n",
    "    tokens = np.append(tokens, next_tok)\n",
    "    if next_tok == eos: break\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\"<bos>\"] + list(\"Hello World!\") + [\"<eos>\"]\n",
    "tokenizer = {s: i for i, s in enumerate(set(message))}\n",
    "detokenizer = {i: s for s, i in tokenizer.items()}\n",
    "tokenize = lambda s: np.array([tokenizer[c] for c in s])\n",
    "detokenize = lambda a: \"\".join([detokenizer[i] for i in a])\n",
    "\n",
    "t = np.linspace(0, 6, len(tokenizer))  # 6 = 2 pi - epsilon.\n",
    "out_emb = np.stack((np.cos(t), np.sin(t)), axis=1)\n",
    "pos_emb = out_emb[tokenize(message[1:])]\n",
    "output = detokenize(\n",
    "  decode_simpleformer(tokenize([\"<bos>\"]), pos_emb, out_emb, eos=tokenizer[\"<eos>\"])[1:-1])\n",
    "\n",
    "assert output == \"Hello World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_enc_3d(t):\n",
    "    x, y = np.cos(t) / (2 ** .5), np.sin(t) / (6 ** .5)\n",
    "    return np.array([x + y, -x + y, -2 * y]) * (3 ** .5)\n",
    "\n",
    "def pos_enc_3d_array(n):\n",
    "    return np.stack([pos_enc_3d(2 * np.pi * t / n) for t in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = len(message) - 1\n",
    "vocab_size = len(tokenizer)\n",
    "params = base_params(vocab_size, block_size, d_model=3)\n",
    "params['tok_emb'] = params['out_emb'] = pos_enc_3d_array(vocab_size)\n",
    "params['pos_emb'] = params['out_emb'][tokenize(message)[1:]] * 1_000_000\n",
    "\n",
    "output = detokenize(decode(tokenize(['<bos>']), params, eos=tokenizer['<eos>'])[1:-1])\n",
    "\n",
    "assert output == 'Hello World!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_3d_enc_rotation_matrix(theta):\n",
    "    T = np.empty((3, 3))\n",
    "    T[0] = pos_enc_3d(0)\n",
    "    T[1] = pos_enc_3d(1)\n",
    "    T[2] = pos_enc_3d(2)\n",
    "\n",
    "    S = np.empty((3, 3))\n",
    "    S[0] = pos_enc_3d(0 + theta)\n",
    "    S[1] = pos_enc_3d(1 + theta)\n",
    "    S[2] = pos_enc_3d(2 + theta)\n",
    "\n",
    "    return np.linalg.solve(T, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_untied_transformer(ks, vs, vocab_size, block_size, prefix_len, tok_dims):\n",
    "    d_head = tok_dims + 3\n",
    "    n_heads = prefix_len\n",
    "    d_model = d_head  # We don't need the standard d_head * n_heads.\n",
    "\n",
    "    params = base_params(vocab_size, block_size, d_model)\n",
    "\n",
    "    params['pos_emb'] = np.pad(pos_enc_3d_array(block_size), ((0, 0), (tok_dims, 0)))\n",
    "    params['tok_emb'] = np.pad(\n",
    "        layer_norm(np.random.normal(0, 1, (vocab_size, tok_dims))), ((0, 0), (0, 3)))\n",
    "    hash_matrices = np.random.normal(0, 1e-1, (prefix_len, tok_dims, tok_dims))\n",
    "\n",
    "    layer0 = base_layer(n_heads, d_model, 0, d_head)\n",
    "    params['layers'].append(layer0)\n",
    "    for head in range(n_heads):\n",
    "        theta = -head * 2 * np.pi / block_size\n",
    "        layer0['Q'][head, -3:, -3:] = pos_3d_enc_rotation_matrix(theta) * 1e8\n",
    "        layer0['K'][head, -3:, -3:] = np.eye(3)\n",
    "        layer0['V'][head] = np.eye(d_model)\n",
    "        layer0['P'][head, :tok_dims, :tok_dims] = hash_matrices[head].T\n",
    "    layer0['P'][0] -= np.eye(d_model)  # Clean up residual.\n",
    "\n",
    "    causal_mask = np.tril(np.ones((prefix_len, prefix_len)))\n",
    "    for k, v in zip(ks, vs):\n",
    "        x = params['tok_emb'][k] + params['pos_emb'][np.arange(len(k))]\n",
    "        params['out_emb'][v] += transformer_layer(x, **layer0, mask=causal_mask)[-1]\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x, axis=-1):\n",
    "  mx = x.max(axis=axis, keepdims=True)\n",
    "  return np.expand_dims(np.log(np.sum(np.exp(x - mx), axis=axis)), axis=axis) + mx\n",
    "\n",
    "def loss_and_grad(E, M, k, v, y):\n",
    "  V, d = E.shape\n",
    "  x = E[k]  # n, l, d\n",
    "  h = np.einsum('nld,ldo->no', x, M)  # n, d\n",
    "  mean = h.sum(axis=1)[:, None] / (d + 3)\n",
    "  h0 = h - mean\n",
    "  var = ((h0 ** 2).sum(axis=1)[:, None] + 3 * mean ** 2) / (d + 3)  # n, 1\n",
    "  std = var ** 0.5 + 1e-10  # n, 1\n",
    "  h01 = h0 / std  # n, d\n",
    "  z = np.einsum('vd,nd->nv', E, h01)  # n, V\n",
    "  s = softmax(z)  # n, V\n",
    "  loss =  (logsumexp(z) - z)[np.arange(len(z)), v].sum()\n",
    "  \n",
    "  dz = s - y  # n, V\n",
    "  dh01 = np.einsum('nv,vd->nd', dz, E)  # n, d\n",
    "  dstd = np.einsum('nd,nd->n', dh01, h0)[:, None] / (-std ** 2)  # n, 1\n",
    "  dvar = dstd / (2 * (std - 1e-10))  # n, 1\n",
    "  dh0 = dh01 / std + 2 * dvar * h0 / (d + 3)  # n, d\n",
    "  dmean = dvar * 6 / (d + 3) * mean - dh0.sum(axis=1)[:, None]\n",
    "  dh = dh0 + dmean / (d + 3)\n",
    "  dM = np.einsum('no,nld->ldo', dh, x)\n",
    "  dx = np.einsum('no,ldo->nld', dh, M)\n",
    "  dE = np.einsum('nd,nv->vd', h01, dz)\n",
    "  np.add.at(dE, k, dx)\n",
    "  \n",
    "  return loss, (dM, dE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "onehot = lambda t, v: np.eye(v)[t]\n",
    "\n",
    "def optimize(E, M, ks, vs, iters, lr=1e-3):\n",
    "  vocab_size = E.shape[0]\n",
    "  y = onehot(vs, vocab_size)\n",
    "  with tqdm(range(iters)) as pb:\n",
    "    for _ in pb:\n",
    "      E[:] = layer_norm(E)\n",
    "      loss, (dM, dE) = loss_and_grad(E, M, ks, vs, y)\n",
    "      E -= lr * dE\n",
    "      M -= lr * dM\n",
    "      pb.set_description(f'{loss=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lookup_transformer(ks, vs, vocab_size, block_size, tok_dims, optimization_iters):\n",
    "  n, prefix_len = ks.shape\n",
    "  d_head = tok_dims + 3\n",
    "  n_heads = prefix_len\n",
    "  d_model = d_head  # We don't need the standard d_head * n_heads.\n",
    " \n",
    "  params = base_params(vocab_size, block_size, d_model)\n",
    "\n",
    "  params['pos_emb'] = np.pad(pos_enc_3d_array(block_size), ((0, 0), (tok_dims, 0)))\n",
    "\n",
    "  tok_emb = np.random.normal(0, 1e-1, (vocab_size, tok_dims))\n",
    "  hash_matrices = np.random.normal(0, 1e-1, (prefix_len, tok_dims, tok_dims))\n",
    "  optimize(tok_emb, hash_matrices, ks, vs, optimization_iters)\n",
    "  params['tok_emb'] = params['out_emb'] = np.pad(layer_norm(tok_emb), ((0, 0), (0, 3)))\n",
    "\n",
    "  layer0 = base_layer(n_heads, d_model, 0, d_head)\n",
    "  params['layers'].append(layer0)\n",
    "  for head in range(n_heads):\n",
    "    theta = -head * 2 * np.pi / block_size\n",
    "    layer0['Q'][head, -3:, -3:] = pos_3d_enc_rotation_matrix(theta) * 1e8\n",
    "    layer0['K'][head, -3:, -3:] = np.eye(3)\n",
    "    layer0['V'][head] = np.eye(d_model)\n",
    "    layer0['P'][head, :tok_dims, :tok_dims] = hash_matrices[-1 - head].T\n",
    "  layer0['P'][0] -= np.eye(d_model)  # Clean up residual.\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 - Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_block(d):\n",
    "  assert d >= 2, 'Padding blocks need at least 2 dimensions.'\n",
    "  return np.array([(d - 1) ** .5] + [-1 / ((d - 1) ** .5) for i in range(d - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_search_transformer(vocab_size, block_size, prefix_len):\n",
    "    d_head = 3 * prefix_len\n",
    "    n_heads = prefix_len + 1\n",
    "    d_model = d_head + 3\n",
    "\n",
    "    params = base_params(vocab_size, block_size, d_model)\n",
    "    params['pos_emb'] = np.pad(pos_enc_3d_array(block_size), ((0, 0), (d_model - 3, 0)))\n",
    "    ln_padding = np.tile(padding_block(d_model - 6), (vocab_size, 1))\n",
    "    tok_emb = pos_enc_3d_array(vocab_size)\n",
    "    tok_emb = np.concatenate((tok_emb, ln_padding), axis=-1)\n",
    "    params['tok_emb'] = params['out_emb'] = np.pad(tok_emb, ((0, 0), (0, 3)))\n",
    "\n",
    "    # d_head is too large for this layer (except for cleanup head0 we could do with 3 dims).\n",
    "    layer0 = base_layer(n_heads, d_model, 0, d_head)\n",
    "    params['layers'].append(layer0)\n",
    "    for head in range(n_heads):\n",
    "        theta = -head * 2 * np.pi / block_size\n",
    "        layer0['Q'][head, -3:, -3:] = pos_3d_enc_rotation_matrix(theta) * 1e8\n",
    "        layer0['K'][head, -3:, -3:] = np.eye(3)\n",
    "        if head == 0:  # Head0 just cleans up everything except for our own token embedding.\n",
    "            layer0['V'][head, 3:] = np.eye(d_head)\n",
    "            layer0['P'][head, 3:] = -np.eye(d_head) # Clean up residual.\n",
    "        else:\n",
    "            layer0['V'][head, :3, :3] = np.eye(3)\n",
    "            layer0['P'][head, head * 3: (head + 1) * 3, :3] = np.eye(3)\n",
    "\n",
    "    # n_heads is too large for this layer (we could do with 1 head + cleanup).\n",
    "    layer1 = base_layer(n_heads, d_model, 0, d_head)\n",
    "    params['layers'].append(layer1)\n",
    "    layer1['Q'][0, :-3] = np.eye(d_head) * 1e8\n",
    "    layer1['K'][0, 3:] = np.eye(d_head)\n",
    "    layer1['V'][0, :3, :3] = np.eye(3)\n",
    "    layer1['P'][0, :3, :3] = np.eye(3)\n",
    "\n",
    "    # Clean up heads (we could scale head0's P by e.g. 1e7 and drop these):\n",
    "    assert prefix_len >= 2\n",
    "    layer1['Q'][1, :-3] = np.eye(d_head) * 1e8\n",
    "    layer1['K'][1, :-3] = np.eye(d_head)\n",
    "    layer1['V'][1, :-3] = np.eye(d_head)\n",
    "    layer1['P'][1, :-3] = -np.eye(d_head)\n",
    "    layer1['Q'][2, -3:, -3:] = np.eye(3) * 1e8\n",
    "    layer1['K'][2, -3:, -3:] = np.eye(3)\n",
    "    layer1['V'][2, -3:, -3:] = np.eye(3)\n",
    "    layer1['P'][2, -3:, -3:] = -np.eye(3)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_min_transformer(vocab_size, block_size):\n",
    "    params = base_params(vocab_size, block_size, d_model=3)\n",
    "    tok_emb = pos_enc_3d_array(2 * vocab_size + 2)[1:1 + vocab_size]\n",
    "    params['tok_emb'] = params['out_emb'] = tok_emb\n",
    "\n",
    "    layer0 = base_layer(n_heads=1, d_model=3, d_ff=0, d_head=3)\n",
    "    params['layers'].append(layer0)\n",
    "\n",
    "    layer0['Q'][0, :, 0] = pos_enc_3d(np.pi / 2) * 1e8\n",
    "    layer0['K'][0, :, 0] = pos_enc_3d(0)\n",
    "    layer0['V'][0] = np.eye(3)\n",
    "    layer0['P'][0] = np.eye(3) * 1e6\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch(x, mx):\n",
    "    return mx * x / x.max()\n",
    "\n",
    "def powerpoints(n, mx=np.pi):\n",
    "    return stretch(np.cumsum(np.concatenate((np.zeros(1), 1 / 2 ** np.arange(n - 1)))), mx)\n",
    "\n",
    "def nextpoint(ts):\n",
    "    return np.concatenate((ts[1:-1] * 3 / 4 + ts[2:] / 4, np.array([np.pi, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sort_transformer(n=28, block_size=100):\n",
    "    pi = pos_enc_3d(powerpoints(n)).T\n",
    "    qi = pos_enc_3d(nextpoint(powerpoints(n))).T\n",
    "\n",
    "    params = base_params(vocab_size=n, block_size=block_size, d_model=6)\n",
    "\n",
    "    layer0 = base_layer(n_heads=1, d_model=6, d_ff=0, d_head=6)\n",
    "    layer0['Q'][3:, :3] = np.eye(3) * 1e20\n",
    "    layer0['K'][:3, :3] = np.eye(3)\n",
    "    layer0['V'][:] = np.eye(6) * 1e20  # Instead of cleanup.\n",
    "    layer0['P'][:] = np.eye(6)\n",
    "\n",
    "    params['tok_emb'] = params['out_emb'] = np.concatenate((pi, qi), axis=1)\n",
    "    params['layers'].append(layer0)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - Decimal Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP(x0s, a0s):\n",
    "    S = np.sort(x0s)\n",
    "    min_sep = np.min(S[1:] - S[:-1])\n",
    "    M1 = np.array([[1, 1, 1] * len(x0s)])\n",
    "    b1 = np.concatenate([np.array([0, 1, -1]) - x0 / min_sep for x0 in x0s])\n",
    "    M2 = np.stack([\n",
    "            np.concatenate([np.array([-2, 1, 1]) * a0[i] for a0 in a0s])\n",
    "            for i in range(len(a0s[0]))], axis=-1)\n",
    "    b2 = np.array([0] * len(a0s[0]))\n",
    "    return M1 / min_sep, b1, M2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_digit_mod_transformer():\n",
    "    params = base_params(vocab_size=10, block_size=2, d_model=4)\n",
    "\n",
    "    tok_emb = np.empty((10, 4))\n",
    "    for i in range(10):\n",
    "        x = i / 100\n",
    "        a = (-x - 1 + (-3 * x ** 2 - 2 * x + 5) ** 0.5) / 2\n",
    "        tok_emb[i] = np.array([x, 1, a, -x - a - 1])\n",
    "    params['tok_emb'] = params['out_emb'] = tok_emb\n",
    "\n",
    "    layer0 = base_layer(n_heads=1, d_model=4, d_ff=3 * 19, d_head=4)\n",
    "    layer0['V'][0, :2, :2] = np.eye(2) * 1e4  # * 1e4 for messy cleanup.\n",
    "    layer0['V'][0, 0, 0] *= 2 * 100  # * 2 for averaging, * 100 for tok_emb factor.\n",
    "    layer0['P'][0, :2, :2] = np.eye(2)\n",
    "\n",
    "    x0s = [layer_norm(np.array([i * 1e4, 1e4, 0., 0.]))[0] for i in range(19)]\n",
    "    a0s = [tok_emb[i % 10] * 1e10 for i in range(19)]\n",
    "    layer0['M1'][0], layer0['b1'], layer0['M2'], layer0['b2'] = build_MLP(x0s, a0s)\n",
    "\n",
    "    params['layers'].append(layer0)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def build_multi_digit_mod_transformer(n_digits):\n",
    "    vocab_size = 10 + 2  # 10 digits, '+' (10) and '=' (11).\n",
    "    block_size = 2 * n_digits + 2  # n_digits for each number +2 for '+' and '='.\n",
    "    d_model = 7  # 4 for token, 3 for position.\n",
    "    params = base_params(vocab_size, block_size, d_model)\n",
    "\n",
    "    tok_emb = np.empty((vocab_size, 4))\n",
    "    for i in range(10):\n",
    "        x = i / 100\n",
    "        a = (-x - 1 + (-3 * x ** 2 - 2 * x + 5) ** 0.5) / 2\n",
    "        tok_emb[i] = np.array([x, 1, a, -x - a - 1])\n",
    "    tok_emb[10] = tok_emb[11] = tok_emb[0]  # Anything normalized.\n",
    "    tok_emb = np.pad(tok_emb, ((0, 0), (0, 3)))\n",
    "    params['tok_emb'][:] = params['out_emb'][:] = tok_emb\n",
    "    params['pos_emb'][:] = np.pad(pos_enc_3d_array(2 * n_digits + 2), ((0, 0), (4, 0)))\n",
    "\n",
    "    out_range = 2 * (10 ** n_digits) - 1\n",
    "    n_heads = 2 * n_digits + 1  # 1 cleanup head.\n",
    "    layer0 = base_layer(n_heads, d_model, d_ff=3 * out_range, d_head=3)\n",
    "\n",
    "    for head, (unit_pos, digit) in enumerate(product([1, n_digits + 2], range(n_digits))):\n",
    "        theta = -(unit_pos + digit) * 2 * np.pi / block_size\n",
    "        layer0['Q'][head, -3:, -3:] = pos_3d_enc_rotation_matrix(theta) * 1e6\n",
    "        layer0['K'][head, -3:, -3:] = np.eye(3)\n",
    "        layer0['V'][head, 0, 0] = 100 * (10 ** digit)\n",
    "        layer0['P'][head, 0, 0] = 1\n",
    "\n",
    "    # Cleanup head:\n",
    "    layer0['Q'][-1, -3:, -3:] = (pos_3d_enc_rotation_matrix(0) * 1e6)\n",
    "    layer0['K'][-1, -3:, -3:] = np.eye(3)\n",
    "    layer0['V'][-1, 2:4, :2] = -np.eye(2)\n",
    "    layer0['P'][-1, 2:4, :2] = np.eye(2)\n",
    "\n",
    "    x0 = lambda i: np.concatenate((np.array([i, 1., 0, 0]), params['pos_emb'][0, -3:]))\n",
    "    x0s = [layer_norm(x0(i))[0] for i in range(out_range)]\n",
    "    a0s = [tok_emb[i % 10] * 1e10 for i in range(out_range)]\n",
    "    layer0['M1'][0], layer0['b1'], layer0['M2'], layer0['b2'] = build_MLP(x0s, a0s)\n",
    "\n",
    "    params['layers'].append(layer0)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decimal_addition_transformer(n_digits):\n",
    "    vocab_size = 10 + 2  # 10 digits, '+' (10) and '=' (11).\n",
    "    block_size = 2 * n_digits + 2 + n_digits  # An extra n_digits for the result.\n",
    "    d_model = 7  # 4 for token, 3 for position.\n",
    "    params = base_params(vocab_size, block_size, d_model)\n",
    "\n",
    "    tok_emb = np.empty((vocab_size, 4))\n",
    "    for i in range(10):\n",
    "        x = i / 100\n",
    "        a = (-x - 1 + (-3 * x ** 2 - 2 * x + 5) ** 0.5) / 2\n",
    "        tok_emb[i] = np.array([x, 1, a, -x - a - 1])\n",
    "    tok_emb[10] = tok_emb[11] = tok_emb[0]  # Anything normalized.\n",
    "    tok_emb = np.pad(tok_emb, ((0, 0), (0, 3)))\n",
    "    params['tok_emb'][:] = params['out_emb'][:] = tok_emb\n",
    "    params['pos_emb'][:] = np.pad(pos_enc_3d_array(3 * n_digits + 2), ((0, 0), (4, 0)))\n",
    "\n",
    "    out_range = 2 * (10 ** n_digits) - 1\n",
    "    n_heads = 2 * n_digits + 1  # 1 cleanup head.\n",
    "    layer0 = base_layer(n_heads, d_model, d_ff=(n_digits + 1) * 3 * out_range, d_head=3)\n",
    "\n",
    "    for head, (num_pos, digit) in enumerate(product([0, n_digits + 1], range(n_digits))):\n",
    "        layer0['Q'][head, 1] = params['pos_emb'][num_pos + n_digits - 1 - digit, -3:] * 1e6\n",
    "        layer0['K'][head, -3:, -3:] = np.eye(3)\n",
    "        layer0['V'][head, 0, 0] = (10 ** digit) * 100\n",
    "        layer0['P'][head, 0, 0] = 1 / 100\n",
    "\n",
    "    # Cleanup and mix head:\n",
    "    layer0['Q'][-1, -3:, -3:] = (pos_3d_enc_rotation_matrix(0) * 1e6)\n",
    "    layer0['K'][-1, -3:, -3:] = np.eye(3)\n",
    "    layer0['V'][-1, 2:4, :2] = -np.eye(2)\n",
    "    layer0['P'][-1, 2:4, :2] = np.eye(2)\n",
    "    layer0['V'][-1, 0, -1] = -1\n",
    "    layer0['V'][-1, -3, -1] = 1\n",
    "    layer0['P'][-1, 0, -1] = 1\n",
    "\n",
    "    x0 = lambda i, p: np.concatenate((np.array([i / 100 + p[-3], 1., 0, 0]), p))\n",
    "    x0s, a0s = [], []\n",
    "    for i in range(out_range):\n",
    "        for p in params['pos_emb'][-n_digits-1:, -3:]:\n",
    "            x0s.append(layer_norm(x0(i, p))[0])\n",
    "        for digit in range(n_digits, -1, -1):\n",
    "            a0s.append(tok_emb[i // (10 ** digit) % 10] * 1e10)\n",
    "\n",
    "    layer0['M1'][0], layer0['b1'], layer0['M2'], layer0['b2'] = build_MLP(x0s, a0s)\n",
    "\n",
    "    params['layers'].append(layer0)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - The OG Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def og_embedding(block_size, d):\n",
    "    t = np.arange(block_size)[:, None] / (10_000 ** (2 * np.arange(d // 2) / d))[None, :]\n",
    "    res = np.empty((block_size, d))\n",
    "    res[:, 0::2] = np.sin(t)\n",
    "    res[:, 1::2] = np.cos(t)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_encoding(q, thetas):\n",
    "  return np.exp(np.arange(q)[:, None] * 1j * thetas[None, :]).view(dtype=np.float_)\n",
    "\n",
    "def reversed_rotation_encoding(q, thetas):\n",
    "  vs = np.exp(np.arange(q)[:, None] * 1j * thetas[None, :])\n",
    "  res = np.empty((q, thetas.shape[0] * 2))\n",
    "  res[:, 0::2] = vs.imag\n",
    "  res[:, 1::2] = vs.real\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_by_delta(d, delta, theta):\n",
    "  M = np.zeros((d, d))\n",
    "  for i in range(d // 2):\n",
    "    M[i * 2,     i * 2]     =  np.cos(theta[i] * delta)\n",
    "    M[i * 2 + 1, i * 2]     = -np.sin(theta[i] * delta)\n",
    "    M[i * 2,     i * 2 + 1] =  np.sin(theta[i] * delta)\n",
    "    M[i * 2 + 1, i * 2 + 1] =  np.cos(theta[i] * delta)\n",
    "  return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_angles = lambda d: 1 / (10_000 ** (2 * np.arange(d // 2) / d))\n",
    "\n",
    "q = 64\n",
    "d = 128\n",
    "np.testing.assert_allclose(reversed_rotation_encoding(q, og_angles(d)), og_embedding(q, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation_embedding(q, d):\n",
    "    return rotation_encoding(q, thetas=np.random.uniform(0, 2 * np.pi, size=d // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_dimension(encoding, epsilon=1e-3):\n",
    "  return np.sum(np.max(encoding, axis=0) - np.min(encoding, axis=0) <= 2 * epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_normal_encoding(q, d):\n",
    "    return np.random.normal(0, 1, (q, d))\n",
    "\n",
    "def random_sign_encoding(q, d):\n",
    "  return np.random.randint(0, 2, (q, d)) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "all_bit_strings = lambda n: list(itertools.product([0, 1], repeat=n))\n",
    "\n",
    "def hamming_embedding(r):\n",
    "    n = 2 ** r - 1\n",
    "    k = n - r\n",
    "    bits = [x for x in all_bit_strings(r) if x != (0,) * r]\n",
    "    bits = sorted(reversed(bits), key=lambda s: s.count(1))  # Put in standard form.\n",
    "    H = np.array(bits).T\n",
    "    A = H[:, r:]\n",
    "    G = np.concatenate((np.eye(k), A.T), axis=1)\n",
    "    bits = np.array(all_bit_strings(k))\n",
    "    return (np.einsum('kn,vk->vn', G, bits) % 2) * 2 - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 - The TAOTP Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def compile_TAOTP(program: str):\n",
    "    params = ast.literal_eval(program)\n",
    "    if 'out_emb' not in params:\n",
    "        params['out_emb'] = params['tok_emb']\n",
    "    for emb in ['tok_emb', 'pos_emb', 'out_emb']:\n",
    "        params[emb] = np.array(params[emb])\n",
    "    for layer in params['layers']:\n",
    "        for k, lst in layer.items():\n",
    "            layer[k] = np.array(lst)\n",
    "    return params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
